{
      "cells": [
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Import necessary libraries\n",
                        "import tensorflow as tf\n",
                        "import matplotlib.pyplot as plt\n",
                        "from keras.applications import EfficientNetB0\n",
                        "from keras.models import Model\n",
                        "from keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, Rescaling, Input, Resizing\n",
                        "from keras.optimizers import Adam\n",
                        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
                        "from keras.models import load_model\n",
                        "from keras.utils import plot_model\n",
                        "\n",
                        "print(tf.__version__)\n",
                        "\n",
                        "# Check GPU SupportS\n",
                        "print(\"\\nIs GPU Supported?\")\n",
                        "print(tf.test.is_built_with_gpu_support())\n",
                        "\n",
                        "# Check TensorFlow Detected Devices\n",
                        "print(\"\\nTensorFlow Detected Devices:\")\n",
                        "print(tf.config.list_physical_devices())\n",
                        "\n",
                        "# Check CUDA availability\n",
                        "print(\"\\nCUDA Available:\")\n",
                        "print(tf.test.is_built_with_cuda())\n",
                        "print()"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Define the directories for train, test, and validation sets\n",
                        "train_dir = 'dataset/train'\n",
                        "test_dir = 'dataset/test'\n",
                        "val_dir = 'dataset/val'"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Define batch size and image dimensions\n",
                        "batch_size = 128\n",
                        "img_height = None\n",
                        "img_width = None\n",
                        "AUTOTUNE = tf.data.AUTOTUNE"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Load and preprocess the datasets\n",
                        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
                        "  train_dir,\n",
                        "  shuffle=True,\n",
                        "  label_mode='binary',\n",
                        "  seed=123,\n",
                        "  batch_size=batch_size)\n",
                        "\n",
                        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
                        "  train_dir,\n",
                        "  shuffle=True,\n",
                        "  label_mode='binary',\n",
                        "  seed=123,\n",
                        "  batch_size=batch_size)\n",
                        "\n",
                        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
                        "  test_dir,\n",
                        "  shuffle=True,\n",
                        "  label_mode='binary',\n",
                        "  seed=123,\n",
                        "  batch_size=410)\n",
                        "\n",
                        "class_names = train_ds.class_names\n",
                        "label_map = {class_names.index('fire'): 1, class_names.index('nofire'): 0}\n",
                        "\n",
                        "# Define a function to remap the labels\n",
                        "def remap_labels(image, label):\n",
                        "    return image, tf.where(label == 0, 1, 0)  # this will swap 0s and 1s\n",
                        "\n",
                        "# Use the `map` function to apply the remap_labels function\n",
                        "train_ds = train_ds.map(remap_labels)\n",
                        "val_ds = val_ds.map(remap_labels)\n",
                        "test_ds = test_ds.map(remap_labels)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Configure dataset for performance\n",
                        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
                        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
                        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Define a function to create model with unified memory\n",
                        "def create_model(base_model):\n",
                        "    # Freeze the convolutional base\n",
                        "    base_model.trainable = True\n",
                        "    fine_tune_at = 1\n",
                        "    for layer in base_model.layers[:fine_tune_at]:\n",
                        "        layer.trainable = False\n",
                        "\n",
                        "    # Add custom layers on top of the pre-trained model\n",
                        "    inputs = Input(shape=(None, None, 3))\n",
                        "    \n",
                        "    x = Rescaling(1./255)(inputs) # Rescale the input RGB values to [0,1] as normalization\n",
                        "    x = Resizing(256, 256, \"bilinear\", True)(x)\n",
                        "    x = base_model(x, training = True)\n",
                        "    x = GlobalAveragePooling2D()(x)\n",
                        "    x = Dense(512, activation='relu')(x)\n",
                        "    x = BatchNormalization()(x)\n",
                        "    x = Dense(128, activation='relu')(x)\n",
                        "    x = BatchNormalization()(x)\n",
                        "    output = Dense(1, activation='sigmoid')(x)\n",
                        "\n",
                        "    # Create the new model\n",
                        "    model = Model(inputs=inputs, outputs=output)\n",
                        "\n",
                        "    # Compile the model\n",
                        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
                        "                  loss=\"binary_crossentropy\",\n",
                        "                  metrics=['accuracy'])\n",
                        "    \n",
                        "    return model"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Model names for saving\n",
                        "model_names = ['EfficientNetB0']\n",
                        "models = [EfficientNetB0]"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Train models\n",
                        "for model_name, base_model in zip(model_names, models):\n",
                        "    print(f\"Training {model_name} model...\")\n",
                        "    \n",
                        "    # Load the pre-trained model without the top (fully connected layers)\n",
                        "    base_model = base_model(weights='imagenet', include_top=False, input_shape=(None, None, 3))\n",
                        "    \n",
                        "    # Create model\n",
                        "    with tf.device('/device:GPU:0'):  # Use GPU for model creation\n",
                        "        model = create_model(base_model)\n",
                        "        plot_model(model, to_file=f'{model_name}_architecture.png', show_shapes=True, show_layer_names=True, show_layer_activations=True, show_trainable=True)\n",
                        "        plot_model(base_model, to_file=f'{model_name}_base_architecture.png', show_shapes=True, show_layer_names=True, show_layer_activations=True, show_trainable=True)\n",
                        "\n",
                        "    # Print model summary\n",
                        "    model.summary()\n",
                        "\n",
                        "    # Define the callbacks\n",
                        "    checkpoint = ModelCheckpoint(f'best_model_{model_name}.h5', monitor='val_loss', save_best_only=True)\n",
                        "    early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
                        "\n",
                        "    # Fit the model with callbacks and validation data\n",
                        "    with tf.device('/device:GPU:0'):  # Use GPU for model training\n",
                        "        hist = model.fit(train_ds, epochs=100, validation_data=val_ds, callbacks=[checkpoint, early_stopping])\n",
                        "\n",
                        "    print(f\"Training {model_name} model completed.\")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Plotting loss\n",
                        "fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
                        "\n",
                        "# Plot training loss\n",
                        "axes[0].plot(hist.history['loss'], color='teal', label='Training Loss')\n",
                        "axes[0].plot(hist.history['val_loss'], color='orange', label='Validation Loss')\n",
                        "axes[0].set_title(f'{model_name} Loss', fontsize=20)\n",
                        "axes[0].legend(loc=\"upper right\")\n",
                        "\n",
                        "# Plot accuracy\n",
                        "axes[1].plot(hist.history['accuracy'], color='blue', label='Training Accuracy')\n",
                        "axes[1].plot(hist.history['val_accuracy'], color='red', label='Validation Accuracy')\n",
                        "axes[1].set_title(f'{model_name} Accuracy', fontsize=20)\n",
                        "axes[1].legend(loc=\"lower right\")\n",
                        "\n",
                        "plt.tight_layout()\n",
                        "plt.savefig(f'{model_name}_training_hist.png')"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 28,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "WARNING:tensorflow:6 out of the last 744 calls to <function Model.make_test_function.<locals>.test_function at 0x000001EE8A1A2CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
                                    "1/1 [==============================] - 8s 8s/step - loss: 0.2710 - accuracy: 0.9098\n",
                                    "Testing EfficientNetB0 model:\n",
                                    "Test Loss (EfficientNetB0): 0.2709668278694153\n",
                                    "Test Accuracy (EfficientNetB0): 0.9097561240196228\n",
                                    "-----------------------------\n"
                              ]
                        }
                  ],
                  "source": [
                        "# Evaluate the models\n",
                        "for model_name in model_names:\n",
                        "    # Load the best model\n",
                        "    best_model = load_model(f'best_model_{model_name}.h5')\n",
                        "\n",
                        "    # Evaluate the model on the validation data\n",
                        "    val_loss, val_accuracy = best_model.evaluate(test_ds)\n",
                        "\n",
                        "    # Print the validation loss and accuracy\n",
                        "    print(f\"Testing {model_name} model:\")\n",
                        "    print(f\"Test Loss ({model_name}):\", val_loss)\n",
                        "    print(f\"Test Accuracy ({model_name}):\", val_accuracy)\n",
                        "    print(\"-----------------------------\")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Define the image augmentation layer\n",
                        "data_augmentation = tf.keras.Sequential([\n",
                        "  tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
                        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
                        "  tf.keras.layers.experimental.preprocessing.RandomZoom(0.1),\n",
                        "  tf.keras.layers.experimental.preprocessing.RandomContrast(0.1),\n",
                        "  tf.keras.layers.experimental.preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1, fill_mode='reflect'),\n",
                        "])\n",
                        "\n",
                        "# Add the augmentation layer to your datasets\n",
                        "# Note: Data augmentation should be applied to the training and possibly the validation dataset but not the test dataset\n",
                        "def augment_images(image, label):\n",
                        "    image = data_augmentation(image)\n",
                        "    return image, label\n",
                        "\n",
                        "train_ds = train_ds.map(augment_images)\n",
                        "val_ds = val_ds.map(augment_images)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Configure dataset for performance\n",
                        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
                        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
                        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "for model_name, base_model in zip(model_names, models):\n",
                        "    print(f\"Fine Tuning {model_name} model...\")\n",
                        "\n",
                        "    with tf.device('/device:GPU:0'):  # Use GPU for model creation\n",
                        "        model = load_model(f'best_model_{model_name}.h5')\n",
                        "\n",
                        "        # Compile the model\n",
                        "        model.compile(optimizer=Adam(learning_rate=5e-5),\n",
                        "                    loss=\"binary_crossentropy\",\n",
                        "                    metrics=['accuracy'])\n",
                        "\n",
                        "    # Define the callbacks\n",
                        "    checkpoint = ModelCheckpoint(f'fine_tuned_model_{model_name}.h5', monitor='val_loss', save_best_only=True)\n",
                        "    early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
                        "\n",
                        "    # Fit the model with callbacks and validation data\n",
                        "    with tf.device('/device:GPU:0'):  # Use GPU for model training\n",
                        "        hist = model.fit(train_ds, epochs=100, validation_data=val_ds, callbacks=[checkpoint, early_stopping])\n",
                        "\n",
                        "    print(f\"Fine Tuning Trained {model_name} model completed.\")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Plotting loss\n",
                        "fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
                        "\n",
                        "# Plot training loss\n",
                        "axes[0].plot(hist.history['loss'], color='teal', label='Training Loss')\n",
                        "axes[0].plot(hist.history['val_loss'], color='orange', label='Validation Loss')\n",
                        "axes[0].set_title(f'{model_name} Tuning Loss', fontsize=20)\n",
                        "axes[0].legend(loc=\"upper right\")\n",
                        "\n",
                        "# Plot accuracy\n",
                        "axes[1].plot(hist.history['accuracy'], color='blue', label='Training Accuracy')\n",
                        "axes[1].plot(hist.history['val_accuracy'], color='red', label='Validation Accuracy')\n",
                        "axes[1].set_title(f'{model_name} Tuning Accuracy', fontsize=20)\n",
                        "axes[1].legend(loc=\"lower right\")\n",
                        "\n",
                        "plt.tight_layout()\n",
                        "plt.savefig(f'{model_name}_fine_tuning_hist.png')"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 42,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "1/1 [==============================] - 8s 8s/step - loss: 0.2029 - accuracy: 0.9317\n",
                                    "Testing EfficientNetB0 model:\n",
                                    "Test Loss (EfficientNetB0): 0.2028810828924179\n",
                                    "Test Accuracy (EfficientNetB0): 0.9317073225975037\n",
                                    "-----------------------------\n"
                              ]
                        }
                  ],
                  "source": [
                        "# Evaluate the models\n",
                        "for model_name in model_names:\n",
                        "    # Load the best model\n",
                        "    best_model = load_model(f'fine_tuned_model_{model_name}.h5')\n",
                        "\n",
                        "    # Evaluate the model on the validation data\n",
                        "    val_loss, val_accuracy = best_model.evaluate(test_ds)\n",
                        "\n",
                        "    # Print the validation loss and accuracy\n",
                        "    print(f\"Testing {model_name} model:\")\n",
                        "    print(f\"Test Loss ({model_name}):\", val_loss)\n",
                        "    print(f\"Test Accuracy ({model_name}):\", val_accuracy)\n",
                        "    print(\"-----------------------------\")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 37,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "IMG_SIZE = (224, 224)\n",
                        "BATCH_SIZE = 32"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 38,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Found 132 files belonging to 2 classes.\n"
                              ]
                        }
                  ],
                  "source": [
                        "dataset_path = './Brazil'  # Adjusted for clarity\n",
                        "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
                        "    dataset_path,\n",
                        "    shuffle=True,\n",
                        "    batch_size=BATCH_SIZE,\n",
                        "    image_size=IMG_SIZE,\n",
                        "    label_mode='binary'\n",
                        ")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 41,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "5/5 [==============================] - 4s 419ms/step - loss: 3.6741 - accuracy: 0.2652\n",
                                    "Results for Fine Tuned Model: Loss = 3.6740992069244385, Accuracy = 0.2651515007019043\n"
                              ]
                        }
                  ],
                  "source": [
                        "# Evaluate the model on the test dataset.\n",
                        "evaluation_loss, evaluation_accuracy = best_model.evaluate(test_dataset)\n",
                        "\n",
                        "print(f'Results for Fine Tuned Model: Loss = {evaluation_loss}, Accuracy = {evaluation_accuracy}')"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Extract images and labels from the generators\n",
                        "test_images, test_labels = next(iter(test_ds))"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# predict the images\n",
                        "pred = best_model.predict(test_images)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def plot_prediction(images, predictions, true_labels_tensor):\n",
                        "    # Convert TensorFlow tensor of true labels to numpy array\n",
                        "    true_labels = true_labels_tensor.numpy()\n",
                        "    \n",
                        "    # Convert predictions to binary values (0 or 1)\n",
                        "    predictions_binary = (predictions > 0.5).astype(int)\n",
                        "    \n",
                        "    num_rows = 5\n",
                        "    num_cols = 5\n",
                        "    num_images = min(num_rows * num_cols, len(images))  # Ensure we don't exceed images length\n",
                        "    plt.figure(figsize=(2*num_cols, num_rows))\n",
                        "    \n",
                        "    for i in range(num_images):\n",
                        "        plt.subplot(num_rows,num_cols, i+1)\n",
                        "        plt.grid(False)\n",
                        "        plt.xticks([])\n",
                        "        plt.yticks([])\n",
                        "        \n",
                        "        # Normalize and plot image\n",
                        "        img = images[i]\n",
                        "        img = img / 255.0\n",
                        "\n",
                        "        plt.imshow(img, aspect='auto')\n",
                        "        \n",
                        "        predicted_label = int(predictions_binary[i].item())\n",
                        "        true_label = int(true_labels[i].item())\n",
                        "        \n",
                        "        if predicted_label == true_label:\n",
                        "            color = 'green'\n",
                        "        else:\n",
                        "            color = 'red'\n",
                        "        \n",
                        "        # Ensure indexing with integer for class names\n",
                        "        plt.xlabel(f\"Pred: {class_names[predicted_label]} (True: {class_names[true_label]})\", color=color)\n",
                        "    plt.tight_layout()\n",
                        "    plt.show()"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "plot_prediction(test_images, pred, test_labels)"
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "Python 3",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.11.5"
            }
      },
      "nbformat": 4,
      "nbformat_minor": 2
}
